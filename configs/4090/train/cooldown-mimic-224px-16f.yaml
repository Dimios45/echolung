# =============================================================================
# EchoJEPA ViT-L Cooldown on MIMIC-IV-Echo -- RTX 4090 (24GB) Single GPU
# =============================================================================
# Usage:
#   python -m app.main --fname configs/4090/train/cooldown-mimic-224px-16f.yaml --devices cuda:0
#
# Run AFTER pretraining completes. Update anneal_ckpt to your best pretrain checkpoint.
# =============================================================================

app: vjepa
nodes: 1
tasks_per_node: 1
cpus_per_task: 8
mem_per_gpu: 22G

folder: experiments/cooldown/mimic_vitl_224px_16f

data:
  dataset_type: VideoDataset
  datasets:
  - data/csv/mimic_pretrain.csv
  datasets_weights: [1.0]
  batch_size: 16
  crop_size: 224
  patch_size: 16
  dataset_fpcs: [16]
  fps: 8
  tubelet_size: 2
  num_workers: 8
  persistent_workers: true
  pin_mem: true

data_aug:
  auto_augment: false
  motion_shift: false
  random_resize_aspect_ratio: [0.9, 1.1]
  random_resize_scale: [0.5, 1.0]
  reprob: 0.0

loss:
  loss_exp: 1.0

mask:
- aspect_ratio: [0.75, 1.5]
  full_complement: false
  max_keep: null
  max_temporal_keep: 1.0
  num_blocks: 8
  spatial_scale: [0.15, 0.15]
  temporal_scale: [1.0, 1.0]
- aspect_ratio: [0.75, 1.5]
  full_complement: false
  max_keep: null
  max_temporal_keep: 1.0
  num_blocks: 2
  spatial_scale: [0.7, 0.7]
  temporal_scale: [1.0, 1.0]

meta:
  dtype: bfloat16
  eval_freq: 100
  load_checkpoint: true
  read_checkpoint: null
  save_every_freq: 5
  seed: 239
  use_sdpa: true

model:
  model_name: vit_large
  pred_depth: 12
  pred_embed_dim: 384
  pred_num_heads: 12
  uniform_power: true
  use_activation_checkpointing: true
  use_mask_tokens: true
  num_mask_tokens: 10
  use_rope: true
  zero_init_mask_tokens: true

optimization:
  is_anneal: true
  resume_anneal: true
  force_load_pretrain: true

  # UPDATE THIS: point to your last pretrain checkpoint (e.g., e240.pt or latest.pt)
  anneal_ckpt: experiments/pretrain/mimic_vitl_224px_16f/latest.pt

  ema: [0.99925, 0.99925]
  weight_decay: 0.04
  final_weight_decay: 0.04

  ipe: 300
  ipe_scale: 1.0

  # Linear decay from pretrain LR down to near-zero
  start_lr: 3.0e-5
  lr: 3.0e-5
  final_lr: 1.0e-6

  warmup: 0
  epochs: 80
